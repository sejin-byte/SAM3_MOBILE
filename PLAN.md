# SAM 3 í†µí•© ì‹¤í–‰ ê³„íšì„œ (Integrated Implementation Plan)

> **í”„ë¡œì íŠ¸**: Meta SAM 3 â†’ ëª¨ë°”ì¼ ë””ë°”ì´ìŠ¤ ë°°í¬
> **íƒ€ê²Ÿ í•˜ë“œì›¨ì–´**: Mac Mini M4 Pro (24GB UMA) â€” ê°œë°œ í™˜ê²½
> **ë°°í¬ íƒ€ê²Ÿ**: iOS (CoreML/ANE), Android (QNN/Hexagon NPU)
> **í•µì‹¬ ì „ëµ**: EfficientSAM3 PHD(Progressive Hierarchical Distillation) + TorchAO ì–‘ìí™” + ExecuTorch ë°°í¬
> **ì›ë³¸ ëª¨ë¸**: SAM 3 â€” 848M íŒŒë¼ë¯¸í„°, Promptable Concept Segmentation (PCS)

---

## 1ë‹¨ê³„: í™˜ê²½ êµ¬ì¶• (Environment Setup)

### 1.1 macOS ê°œë°œ ë„êµ¬ ì„¤ì¹˜

- [x] **Homebrew ì„¤ì¹˜** â€” v5.0.13 âœ…
  ```bash
  /bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"
  ```

- [x] **Node.js ì„¤ì¹˜** â€” v25.5.0 âœ…
  ```bash
  brew install node
  node -v
  ```

- [x] **Miniforge ì„¤ì¹˜** â€” conda v25.11.0 âœ…
  ```bash
  brew install miniforge
  conda init zsh
  source ~/.zshrc
  ```

### 1.2 AI ì—ì´ì „íŠ¸ ë„êµ¬ ì„¤ì •

- [x] **Claude Code ì„¤ì¹˜ ë° ì¸ì¦** âœ…
  ```bash
  brew install --cask claude-code
  claude login
  ```
  > í„°ë¯¸ë„ ëª…ë ¹ì–´ ììœ¨ ì‹¤í–‰ ê¶Œí•œì„ Allow ëª¨ë“œë¡œ ì„¤ì •í•˜ì—¬ ì‘ì—… ì†ë„ í–¥ìƒ

- [x] **Google Antigravity ì„¤ì¹˜** âœ…
  - `antigravity.google/download` ì—ì„œ Apple Siliconìš© `.dmg` ë‹¤ìš´ë¡œë“œ
  - Google ê³„ì • ë¡œê·¸ì¸ í›„ Mission Control Setup ì™„ë£Œ

- [x] **OpenAI Codex CLI ì„¤ì¹˜** âœ…
  ```bash
  npm install -g @openai/codex
  codex login
  ```

- [x] **Antigravity-Claude í”„ë¡ì‹œ ì„¤ì •** â€” v2.6.2 âœ…
  ```bash
  npm install -g antigravity-claude-proxy
  antigravity-claude-proxy start
  ```
  > Antigravity ì„¤ì •ì—ì„œ ëª¨ë¸ ì—”ë“œí¬ì¸íŠ¸ë¥¼ `http://localhost:8080`ìœ¼ë¡œ ì§€ì •

### 1.3 Python ê°€ìƒí™˜ê²½ ë° ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬

- [x] **conda ê°€ìƒí™˜ê²½ ìƒì„±** â€” `sam3_mobile` (Python 3.10) âœ…
  ```bash
  conda create -n sam3_mobile python=3.10
  conda activate sam3_mobile
  ```

- [x] **PyTorch Nightly ì„¤ì¹˜** â€” v2.11.0.dev20260207, MPS âœ…
  ```bash
  pip install --pre torch torchvision --index-url https://download.pytorch.org/whl/nightly/cpu
  ```
  > MPS ê°€ì† í™•ì¸:
  > ```python
  > import torch
  > print(torch.backends.mps.is_available())  # True
  > ```

- [x] **HuggingFace Transformers ì„¤ì¹˜** â€” v5.2.0.dev0, Sam3Model í¬í•¨ âœ…
  ```bash
  pip install git+https://github.com/huggingface/transformers.git
  ```
  > ê³µì‹ SAM 3 ë¦¬í¬ì§€í† ë¦¬ëŠ” Triton/CUDA ì˜ì¡´ì„±ì´ ìˆì–´ Apple Siliconì—ì„œ ì§ì ‘ ì‚¬ìš© ë¶ˆê°€.
  > HuggingFace êµ¬í˜„ì²´ë¡œ ìš°íšŒí•˜ì—¬ MPS ë°±ì—”ë“œì—ì„œ ì¶”ë¡  ê°€ëŠ¥.

- [x] **ExecuTorch ì„¤ì¹˜** âœ…
  ```bash
  pip install executorch
  ./install_requirements.sh --pybind coreml
  ```

- [x] **TorchAO (PyTorch Architecture Optimization)** â€” v0.17.0 (ì†ŒìŠ¤ ë¹Œë“œ) âœ…
  ```bash
  # torch 2.11.0.dev í˜¸í™˜ì„ ìœ„í•´ ì†ŒìŠ¤ì—ì„œ ì„¤ì¹˜
  pip install --no-build-isolation git+https://github.com/pytorch/ao.git
  # API ë³€ê²½: int4_weight_only() â†’ Int4WeightOnlyConfig
  ```

### 1.4 SAM 3 ì›ë³¸ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ë² ì´ìŠ¤ë¼ì¸ ì¶”ë¡  í™•ì¸

- [x] **SAM 3 ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ë‹¤ìš´ë¡œë“œ** â€” 840.4M params, FP16 1,681MB âœ…
  ```python
  from transformers import Sam3Model, Sam3Processor

  # facebook/sam3 gated repo ì ‘ê·¼ ìŠ¹ì¸ ì™„ë£Œ
  processor = Sam3Processor.from_pretrained("facebook/sam3")
  model = Sam3Model.from_pretrained("facebook/sam3", torch_dtype=torch.float16)
  ```

- [x] **MPS ë””ë°”ì´ìŠ¤ ì¶”ë¡  í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸** (`run_sam3.py`) ì‘ì„± âœ…
  ```python
  import torch

  device = "mps" if torch.backends.mps.is_available() else "cpu"
  model = model.to(device)

  # DataLoaderì—ì„œ pin_memory=False í•„ìˆ˜ (MPS í˜¸í™˜ì„±)
  # device_map="auto" ëŒ€ì‹  ëª…ì‹œì ìœ¼ë¡œ device="mps" ì§€ì •
  ```
  > **íŠ¸ëŸ¬ë¸”ìŠˆíŒ…**:
  > - `RuntimeError: Triton packages are not available` â†’ `device="mps"` ëª…ì‹œ ì§€ì •
  > - `pin_memory` ì¶©ëŒ â†’ `DataLoader(pin_memory=False)` ì„¤ì •

- [x] **ë² ì´ìŠ¤ë¼ì¸ ì¶”ë¡  ê²°ê³¼ ì €ì¥** â€” `outputs/baseline/` âœ…
  - ì¶œë ¥ í‚¤: pred_masks[1,200,288,288], pred_boxes[1,200,4], pred_logits[1,200]
  - ì¡´ì¬ í† í°(Presence Token): presence_logits[1,1]
  - ì‹œë§¨í‹± ë¶„í• : semantic_seg[1,1,288,288]
  - êµì‚¬ ì¶œë ¥: `teacher_outputs.pt` (ì¦ë¥˜ í•™ìŠµ ë°ì´í„°ë¡œ í™œìš©)

### 1.5 ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ë° ì „ì²˜ë¦¬

- [x] **SA-1B ë°ì´í„°ì…‹** â€” 33,558 ì´ë¯¸ì§€ âœ…
  - 3ê°œ tar ì„œë¸Œì…‹: sa_000020, sa_000097, sa_000524 (~33GB)
  - ì €ì¥: `data/sa1b/` â€” ì´ë¯¸ì§€(.jpg) + ì–´ë…¸í…Œì´ì…˜(.json) ìŒ
  - ìš©ë„: ì´ë¯¸ì§€ ì¦ë¥˜ Phase 1 (Feature Alignment) + Phase 2 (Output Refinement)

- [x] **SA-V (Segment Anything Video) ë°ì´í„°ì…‹** â€” 919 ë¹„ë””ì˜¤ âœ…
  - íŒŒì¼: sav_000.tar (~8.10GB)
  - ì €ì¥: `data/sa_v/sav_train/sav_000/` â€” MP4 + ìˆ˜ë™/ìë™ ì–´ë…¸í…Œì´ì…˜ JSON
  - êµ¬ì¡°: `masklet[frame_idx][object_idx]` = RLE {size, counts}, 4í”„ë ˆì„ ê°„ê²© ì–´ë…¸í…Œì´ì…˜
  - ìš©ë„: ë¹„ë””ì˜¤ ì¦ë¥˜ (ì‹œê°„ì  ë©”ëª¨ë¦¬ ëª¨ë“ˆ í•™ìŠµ)

- [x] **SA-Co (Segment Anything with Concepts) ë°ì´í„°ì…‹** â€” Gold + Silver ë‹¤ìš´ë¡œë“œ ì™„ë£Œ âœ…
  - Gold: 465MB (24 íŒŒì¼), Silver: 631MB (13 íŒŒì¼)
  - ì €ì¥: `data/sa_co/gold/`, `data/sa_co/silver/`
  - VEval (32.25GB): 3ë‹¨ê³„ ì¦ë¥˜ ê²€ì¦ ì‹œ ë‹¤ìš´ë¡œë“œ ì˜ˆì •
  - ìš©ë„: 3ë‹¨ê³„ ì¦ë¥˜ (ì—”ë“œ-íˆ¬-ì—”ë“œ PCS ë¯¸ì„¸ ì¡°ì • ë° ê²€ì¦)

- [x] **ë°ì´í„°ì…‹ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•** â€” `data/prepare_datasets.py` âœ…
  - SA1BDataset, SAVDataset, SACoDataset í´ë˜ìŠ¤ êµ¬í˜„
  - DataLoader êµ¬ì„± (pin_memory=False, batch_size=1)
  - ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„± ì™„ë£Œ
  - `python data/prepare_datasets.py --verify` ë¡œ ìƒíƒœ ê²€ì¦ ê°€ëŠ¥

---

## 2ë‹¨ê³„: ì•„í‚¤í…ì²˜ ê²½ëŸ‰í™” (RepViT Student Model)

> **ëª©í‘œ**: SAM 3ì˜ 848M íŒŒë¼ë¯¸í„°ë¥¼ ëª¨ë°”ì¼ ì¹œí™”ì ì¸ ê²½ëŸ‰ êµ¬ì¡°ë¡œ êµì²´.
> ë‹¨ìˆœíˆ ë ˆì´ì–´ ìˆ˜ë¥¼ ì¤„ì¸ ViTê°€ ì•„ë‹ˆë¼, ëª¨ë°”ì¼ NPUì˜ ë©”ëª¨ë¦¬ ì ‘ê·¼ íŒ¨í„´ê³¼ ìºì‹œ íš¨ìœ¨ì„±ì„ ê³ ë ¤í•œ ì•„í‚¤í…ì²˜ ì„ ì •.

### 2.1 ì´ë¯¸ì§€ ì¸ì½”ë”: RepViT-M2.3 ë°±ë³¸

- [x] **RepViT-M2.3 ì•„í‚¤í…ì²˜ ì„ ì • ë° êµ¬í˜„** â€” `models/backbone_repvit.py` âœ…
  - êµ¬ì¡°ì  ì¬ë§¤ê°œë³€ìˆ˜í™”(Structural Re-parameterization) ê¸°ë°˜
  - timm `repvit_m2_3` (features_only=True) + FPN ì±„ë„ ì–´ëŒ‘í„° (80/160/320/640â†’256)
  - Feature map sizes at 1008x1008: 252x252, 126x126, 63x63, 32x32

- [x] **ImageNet ì‚¬ì „ í•™ìŠµ ê°€ì¤‘ì¹˜ ë¡œë“œ** âœ…
  - `timm.create_model('repvit_m2_3', pretrained=True, features_only=True)`
  - 25.07M params

- [x] **SAM 3ì˜ Perception Encoderë¥¼ RepViT-M2.3ìœ¼ë¡œ êµì²´** âœ…
  - FPN ì±„ë„ ì–´ëŒ‘í„°: Conv1x1 + BN + ReLU + Conv3x3 + BN + ReLU
  - SinePositionEmbedding per level

### 2.2 í…ìŠ¤íŠ¸ ì¸ì½”ë”: MobileCLIP-S1

- [x] **MobileCLIP-S1 ëª¨ë¸ í†µí•©** â€” `models/text_encoder_mobileclip.py` âœ…
  - open_clip `MobileCLIP-S1` (pretrained='datacompdr'), 12-layer TextTransformer
  - ì‹œí€€ìŠ¤ ì „ì²´ hidden states ì¶”ì¶œ (pooledë§Œì´ ì•„ë‹˜)
  - 63.30M params

- [x] **í…ìŠ¤íŠ¸ ì„ë² ë”© ì°¨ì› í˜¸í™˜ì„± ê²€ì¦** âœ…
  - nn.Linear(512, 256) í”„ë¡œì ì…˜ìœ¼ë¡œ hidden_size ì •ë ¬
  - ì¶œë ¥: [batch, seq_len, 256]

### 2.3 ë¹„ë””ì˜¤ ë©”ëª¨ë¦¬ ëª¨ë“ˆ: Perceiver Resampler ê¸°ë°˜ ì••ì¶• ë©”ëª¨ë¦¬

- [x] **Perceiver Resampler ëª¨ë“ˆ êµ¬í˜„** â€” `models/perceiver_resampler.py` âœ…
  - K=64 learnable latent tokens, 2-layer cross-attention + FFN (pre-norm)
  - nn.MultiheadAttention(batch_first=True) for ExecuTorch compatibility
  - 1.60M params

- [x] **SAM 3ì˜ Dense Memory Bankë¥¼ Perceiver ì••ì¶• ë©”ëª¨ë¦¬ë¡œ ëŒ€ì²´** âœ…
  - ê³ ì • ì¶œë ¥ shape [batch, 64, 256] â€” ë¹„ë””ì˜¤ ê¸¸ì´ ë¬´ê´€

### 2.4 ë””ì½”ë” ê²½ëŸ‰í™”

- [x] **DETR ì¸ì½”ë”/ë””ì½”ë” ê²½ëŸ‰í™” ì„¤ê³„** â€” `models/lightweight_detr.py` âœ…
  - Encoder: 3 layers (teacher: 6), FFN=1024 (teacher: 2048), 3.16M params
  - Decoder: 3 layers, 100 queries (teacher: 200), FFN=1024, 4.45M params
  - ì¡´ì¬ í—¤ë“œ(Presence Head), Box RPB, iterative box refinement ëª¨ë‘ ìœ ì§€
  - DotProductScoring: 0.66M params

- [x] **ë§ˆìŠ¤í¬ ë””ì½”ë”** â€” `models/mask_decoder.py` âœ…
  - PixelDecoder (3-stage FPN), MaskEmbedder (3-layer MLP), semantic seg head
  - êµì‚¬ì™€ ë™ì¼ êµ¬ì¡° (hidden_size=256), 2.04M params

- [x] **ì •ì  ê·¸ë˜í”„ í˜¸í™˜ì„±** âœ…
  - nn.MultiheadAttention(batch_first=True) ì‚¬ìš© â€” ExecuTorch export í˜¸í™˜
  - ë™ì  ì œì–´ íë¦„ ìµœì†Œí™” (ê³ ì • layer count, ê³ ì • query count)

---

## 3ë‹¨ê³„: ì§€ì‹ ì¦ë¥˜ (Progressive Hierarchical Distillation)

> **ëª©í‘œ**: êµì‚¬ ëª¨ë¸(SAM 3, 848M)ì˜ ì§€ì‹ì„ í•™ìƒ ëª¨ë¸(EfficientSAM3, 100.7M)ì— ë‹¨ê³„ì ìœ¼ë¡œ ì „ì´.
> ì´ë¯¸ì§€ ì¦ë¥˜ â†’ ë¹„ë””ì˜¤ ì¦ë¥˜ â†’ ì—”ë“œ-íˆ¬-ì—”ë“œ ë¯¸ì„¸ ì¡°ì • ìˆœì„œë¡œ ì§„í–‰.

### 3.1 ì´ë¯¸ì§€ ì¦ë¥˜ Phase 1: Feature Alignment (ì¸ì½”ë” íŠ¹ì§• ì •ë ¬)

- [x] **ì¦ë¥˜ ì¸í”„ë¼ êµ¬ì¶•** âœ…
  - `distillation/` íŒ¨í‚¤ì§€: config, dataset, prompt_encoder, greedy_matcher, losses, trainer
  - IoU Head ì¶”ê°€: `DecoderMLP(256, 256, 1, num_layers=3)` â†’ iou_scores[batch, 100]
  - `forward_with_intermediates()`: FPN features, encoder output, decoder hidden states ë°˜í™˜ + prompt injection
  - GeometricPromptEncoder: ì‚¬ì¸ ìœ„ì¹˜ ì¸ì½”ë”© + íƒ€ì… ì„ë² ë”© â†’ 256-dim
  - GreedyMatcher: MPS-native (no scipy), cost = mask_iou + box_l1 + logit_sim, greedy assignment

- [x] **9ê°œ ì†ì‹¤ í•­ëª© ì„¤ê³„ ë° ê²€ì¦** âœ…
  - ì¶œë ¥ ì†ì‹¤ (í•­ìƒ í™œì„±): mask(Dice+BCE), box_L1, box_GIoU, logit, iou_token, presence, semantic_seg
  - íŠ¹ì§• ì†ì‹¤ (Phase 1ë§Œ): fpn_feature(P1), encoder_feature(P1)
  - ëª¨ë“  ì†ì‹¤ MPSì—ì„œ finite í™•ì¸, backward pass ì •ìƒ ì‘ë™

- [x] **Phase 1 í•™ìŠµ ì™„ë£Œ** âœ…
  - ì„¤ì •: 1 epoch, lr=1e-4, warmup=500, batch=4, grad_accum=2
  - ë™ì  í”„ë¡¬í”„íŠ¸: text 50% / point 25% / box 25%
  - 504px í•´ìƒë„ (teacher RoPE resize ì ìš©)
  - ì²´í¬í¬ì¸íŠ¸: `checkpoints/distillation/phase1_epoch0_step8139.pt`

### 3.2 ì´ë¯¸ì§€ ì¦ë¥˜ Phase 2: Output Refinement (ì¶œë ¥ ì •ì œ) â€” ğŸ”„ ì§„í–‰ ì¤‘

- [x] **Phase 2 í•™ìŠµ ì½”ë“œ ì¤€ë¹„** âœ…
  - íŠ¹ì§• ì†ì‹¤ ë¹„í™œì„±í™”, ì¶œë ¥ ì†ì‹¤ë§Œ ì‚¬ìš©
  - lr=5e-5, ë™ì  í”„ë¡¬í”„íŠ¸: text 30% / point 35% / box 35%
  - Phase 1 ì²´í¬í¬ì¸íŠ¸ ìë™ ë¡œë“œ (`strict=False` â€” memory_cross_attn ì‹ ê·œ ëª¨ë“ˆ ëŒ€ì‘)

- [ ] **Phase 2 í•™ìŠµ ì‹¤í–‰**
  ```bash
  python train_distill.py --phase 2 --device mps
  ```
  - 3 epochs, SA-1B 32,558 images
  - ì˜ˆìƒ: ~24ì‹œê°„

### 3.3 ë¹„ë””ì˜¤ ì¦ë¥˜: Temporal Memory (ì‹œê°„ì  ë©”ëª¨ë¦¬ í•™ìŠµ) â€” â³ ì½”ë“œ ì™„ì„±, ìºì‹± ëŒ€ê¸°

> **ì•„í‚¤í…ì²˜**: í•™ìƒ ëª¨ë¸ì˜ Perceiver Resampler (1.6M) + MemoryCrossAttention (0.26M)ë§Œ í•™ìŠµ.
> ë‚˜ë¨¸ì§€ ~98.8M íŒŒë¼ë¯¸í„°ëŠ” ì´ë¯¸ì§€ ì¦ë¥˜ ê²°ê³¼ë¥¼ ë™ê²°(freeze).
> êµì‚¬ FPN L3 íŠ¹ì§•ì„ ì‚¬ì „ ìºì‹±í•˜ì—¬ í•™ìŠµ ì‹œ êµì‚¬ ëª¨ë¸ ë¶ˆí•„ìš”.

```
Context T=8 frames â†’ cached FPN L3 [T, 256, 18, 18] â†’ flatten [T*324, 256]
                                                              â†“
                                                    Perceiver Resampler (TRAINABLE)
                                                              â†“
                                                       [batch, 64, 256]
                                                              â†“
Query frame â†’ Student backbone (FROZEN) â†’ FPN â†’ DETR encoder (FROZEN)
                                                              â†“
                                          MemoryCrossAttention(encoder_out, memory) (TRAINABLE)
                                                              â†“
                                          DETR decoder (FROZEN) â†’ masks, boxes
                                                              â†“
                                          Loss vs SA-V GT masks (Dice+BCE, L1+GIoU)
```

- [x] **MemoryCrossAttention ëª¨ë“ˆ êµ¬í˜„** â€” `models/memory_attention.py` âœ…
  - Pre-norm cross-attention: vision features(Q) Ã— memory tokens(K,V) + gated residual
  - gate=0 ì´ˆê¸°í™”ë¡œ ì´ë¯¸ì§€ ì„±ëŠ¥ ë¬´ì˜í–¥ ë³´ì¥, í•™ìŠµ ì‹œ ì ì§„ì  í™œì„±í™”
  - 264K params, batch_first=True (ExecuTorch í˜¸í™˜)

- [x] **EfficientSAM3.forward_video() êµ¬í˜„** â€” `models/efficient_sam3.py` âœ…
  - Perceiver compress â†’ MemoryCrossAttention â†’ decoder â†’ predictions
  - ì¶œë ¥ shape ê²€ì¦ ì™„ë£Œ: pred_masks[1,100,126,126], pred_boxes[1,100,4] ë“±
  - freeze/unfreeze ê²€ì¦: perceiver_resampler + memory_cross_attnë§Œ requires_grad=True

- [x] **êµì‚¬ FPN L3 ìºì‹± ìŠ¤í¬ë¦½íŠ¸** â€” `cache_teacher_features.py` âœ…
  - 919 SA-V ë¹„ë””ì˜¤ Ã— ~121 í”„ë ˆì„/ë¹„ë””ì˜¤ â†’ FPN level 3 [256, 18, 18] FP16
  - ë°°ì¹˜ ì²˜ë¦¬ (batch=4), resumable (ê¸°ì¡´ ìºì‹œ ìŠ¤í‚µ)
  - ì˜ˆìƒ ë””ìŠ¤í¬: ~12 GB, ì˜ˆìƒ ì‹œê°„: ~16ì‹œê°„

- [ ] **êµì‚¬ íŠ¹ì§• ìºì‹± ì‹¤í–‰**
  ```bash
  python cache_teacher_features.py --device mps
  ```

- [x] **ë¹„ë””ì˜¤ ì¦ë¥˜ íŒ¨í‚¤ì§€ êµ¬í˜„** âœ…
  - `distillation/video_config.py` â€” VideoDistillationConfig
  - `distillation/video_dataset.py` â€” SAVVideoDataset + video_collate_fn
    - í´ë¦½ ìƒ˜í”Œë§: T context (ìºì‹œ) + 1 query (MP4ì—ì„œ ì¶”ì¶œ)
    - GT: pycocotools RLE ë””ì½”ë”© â†’ binary masks + boxes
  - `distillation/video_losses.py` â€” 5ê°œ GT ê¸°ë°˜ ì†ì‹¤ (mask, box_l1, box_giou, iou_token, presence)
  - `distillation/video_trainer.py` â€” freeze/unfreeze, ì½”ì‚¬ì¸ LR, ì²´í¬í¬ì¸íŠ¸ (trainableë§Œ ì €ì¥)
  - `train_video_distill.py` â€” CLI ì§„ì…ì  (ì´ë¯¸ì§€ ì²´í¬í¬ì¸íŠ¸ ìë™ íƒìƒ‰)

- [ ] **ë¹„ë””ì˜¤ ì¦ë¥˜ í•™ìŠµ ì‹¤í–‰**
  ```bash
  python train_video_distill.py --student-ckpt <phase2_checkpoint> --debug  # ìŠ¤ëª¨í¬ í…ŒìŠ¤íŠ¸
  python train_video_distill.py --student-ckpt <phase2_checkpoint>          # í’€ í•™ìŠµ
  ```
  - ì„¤ì •: 5 epochs, lr=1e-4, warmup=200, batch=1, grad_accum=4
  - í•™ìŠµ ê°€ëŠ¥ íŒŒë¼ë¯¸í„°: ~1.86M (Perceiver 1.6M + MemoryCrossAttn 0.26M)

- [ ] **ë¹„ë””ì˜¤ ì¶”ì  ì •í™•ë„ ì¤‘ê°„ ê²€ì¦**
  - ê°€ë ¤ì§(Occlusion) ìƒí™©ì—ì„œì˜ ì¶”ì  ì§€ì†ì„± í™•ì¸
  - ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê³ ì • ì—¬ë¶€ í™•ì¸ (K=64 ìœ ì§€)

### 3.4 Stage 3: ì—”ë“œ-íˆ¬-ì—”ë“œ PCS ë¯¸ì„¸ ì¡°ì • (End-to-End Fine-Tuning)

- [ ] **ë°ì´í„°ì…‹: SA-Co (Segment Anything with Concepts) ë¡œë“œ**

- [ ] **ì „ì²´ íŒŒì´í”„ë¼ì¸ ë™ê²° í•´ì œ(Unfreeze) ë° ë¯¸ì„¸ ì¡°ì •**
  - ë°±ë³¸(RepViT) + í…ìŠ¤íŠ¸ ì¸ì½”ë”(MobileCLIP) + ë©”ëª¨ë¦¬(Perceiver) + ë””ì½”ë”
  - ì¡´ì¬ í—¤ë“œ(Presence Head)ê°€ ê²½ëŸ‰ ë°±ë³¸ íŠ¹ì§• ë§µì— ì ì‘í•˜ë„ë¡ í•™ìŠµ
  - ë¯¸ì„¸í•œ ì˜ë¯¸ë¡ ì  ì°¨ì´ í•™ìŠµ (ì˜ˆ: "ë¹¨ê°„ ì˜· ì‚¬ëŒ" vs "íŒŒë€ ì˜· ì‚¬ëŒ")

- [ ] **QAT ì¤€ë¹„: Fake Quantization ë…¸ë“œ ì‚½ì…** (4ë‹¨ê³„ ì—°ê³„)
  ```python
  from torchao.quantization import quantize_, Int4WeightOnlyConfig
  # ë¯¸ì„¸ ì¡°ì • í›„ë°˜ë¶€ì—ì„œ fake quantization í™œì„±í™”
  ```

### 3.5 ì¤‘ê°„ ê²€ì¦: êµì‚¬ ëª¨ë¸ ëŒ€ë¹„ ì„±ëŠ¥ í™•ì¸

- [ ] **SA-Co ê²€ì¦ ì„¸íŠ¸ í‰ê°€**
  - **ëª©í‘œ: êµì‚¬ ëª¨ë¸ ëŒ€ë¹„ 85% ì´ìƒ ì„±ëŠ¥ ë‹¬ì„±**
  - mIoU (Mean Intersection over Union) ì¸¡ì •
  - Presence Token ì •í™•ë„ (ì¡´ì¬ íŒë‹¨ F1 ìŠ¤ì½”ì–´)
  - ë¹„ë””ì˜¤ ì¶”ì  J&F ìŠ¤ì½”ì–´

- [ ] **ì„±ëŠ¥ ë¯¸ë‹¬ ì‹œ ëŒ€ì‘ ì „ëµ**
  - í•™ìŠµë¥  ì¡°ì • ë° ì¶”ê°€ ì—í¬í¬ í›ˆë ¨
  - ì†ì‹¤ í•¨ìˆ˜ ê°€ì¤‘ì¹˜ ì¬ì¡°ì •
  - ë°±ë³¸ í¬ê¸° ìƒí–¥ ê²€í†  (RepViT-M2.3 â†’ M3.0)

### 3.6 ì‚¬ìš©ì ì‹œê° ê²€ì¦ (Visual QA)

> **ëª©í‘œ**: í•™ìŠµ ì™„ë£Œ í›„, "ì‚¬ìš©ì ê´€ì "ì—ì„œ í•™ìƒ ëª¨ë¸ì´ ì‹¤ì œ ì´ë¯¸ì§€ì— ëŒ€í•´
> í”„ë¡¬í”„íŠ¸ì— ë°˜ì‘í•˜ë©° í•©ë¦¬ì ì¸ ë§ˆìŠ¤í¬ë¥¼ ë‚´ëŠ”ì§€ ë¹ ë¥´ê²Œ ëˆˆìœ¼ë¡œ í™•ì¸.

- [ ] **ì‹œê° ê²€ì¦ ë¦¬í¬íŠ¸ ìƒì„± (ì´ë¯¸ì§€)**
  - ìŠ¤í¬ë¦½íŠ¸: `scripts/visual_eval_student.py`
  - ì‚°ì¶œë¬¼: `outputs/visual_eval/<run>/index.html` + PNG ê°¤ëŸ¬ë¦¬
  - ì˜ˆì‹œ:
    ```bash
    conda activate sam3_mobile

    # í•™ìƒë§Œ (original | student)
    python scripts/visual_eval_student.py \
      --student-ckpt checkpoints/distillation/phase2_epoch2_step<FINAL>.pt \
      --image outputs/baseline/test_image.png \
      --prompt "objects in the image" \
      --top-k 5 \
      --out-dir outputs/visual_eval/phase2_final_student_only

    # êµì‚¬ ë¹„êµ (original | teacher | student) â€” distillationê³¼ ë™ì¼ í† í¬ë‚˜ì´ì € ì‚¬ìš©
    python scripts/visual_eval_student.py \
      --student-ckpt checkpoints/distillation/phase2_epoch2_step<FINAL>.pt \
      --image outputs/baseline/test_image.png \
      --prompt "objects in the image" \
      --top-k 5 \
      --compare-teacher \
      --out-dir outputs/visual_eval/phase2_final_vs_teacher
    ```

- [ ] **(ì„ íƒ) ì‹¤ì œ ì‚¬ìš©ì ì´ë¯¸ì§€ í´ë”ë¡œ ì¼ê´„ í…ŒìŠ¤íŠ¸**
  ```bash
  python scripts/visual_eval_student.py \
    --student-ckpt checkpoints/distillation/phase2_epoch2_step<FINAL>.pt \
    --image-dir <your_images_dir> \
    --prompt "segment everything" \
    --top-k 5 \
    --out-dir outputs/visual_eval/user_images
  ```

- [ ] **ì²´í¬ í¬ì¸íŠ¸(ëˆˆìœ¼ë¡œ í™•ì¸)**
  - ì‘ì€/ì–‡ì€ ë¬¼ì²´, ë‹¤ì¤‘ ê°ì²´, ë°°ê²½(Stuff)ì—ì„œ ëˆ„ë½/ê³¼ë¶„í• /ë°°ê²½ ì˜¤ì—¼ ì—¬ë¶€
  - í”„ë¡¬í”„íŠ¸ ë³€í™”ì— ëŒ€í•œ ë°˜ì‘ì„±(ì˜ˆ: "person", "car", "food" ë“±)
  - teacher ë¹„êµ ì‹œ: top-k ë§ˆìŠ¤í¬ì˜ ëŒ€ëµì  coverageì™€ ë…¸ì´ì¦ˆ ìˆ˜ì¤€

### 3.7 í•™ìŠµ ì¢…ë£Œ í›„ ìš©ëŸ‰ í™•ë³´ (Artifact Pruning)

> **ëª©í‘œ**: ìµœì¢… ì²´í¬í¬ì¸íŠ¸ë§Œ ë‚¨ê¸°ê³  ì¤‘ê°„ ì‚°ì¶œë¬¼(ëŒ€ìš©ëŸ‰)ì„ ì •ë¦¬í•˜ì—¬ ë””ìŠ¤í¬ ìš©ëŸ‰ í™•ë³´.
> (ì •ë¦¬ ì‘ì—…ì€ ëª¨ë¸ ìµœì¢… ì„±ëŠ¥ í™•ì¸ + ë‚´ë³´ë‚´ê¸° ì‚°ì¶œë¬¼ ì €ì¥ í›„ì—ë§Œ ìˆ˜í–‰)

- [ ] **ì •ë¦¬ ì „ ë°˜ë“œì‹œ ë³´ì¡´í•  ê²ƒ**
  - ìµœì¢… ì´ë¯¸ì§€ ì¦ë¥˜ ì²´í¬í¬ì¸íŠ¸: `checkpoints/distillation/phase2_epoch2_step<FINAL>.pt`
  - (ë¹„ë””ì˜¤ ì¦ë¥˜ ì™„ë£Œ ì‹œ) ìµœì¢… ë¹„ë””ì˜¤ ì¦ë¥˜ ì²´í¬í¬ì¸íŠ¸: `checkpoints/video_distillation/video_epoch*_step*.pt`
  - (ì–‘ìí™”/ë°°í¬ ì§„í–‰ ì‹œ) ì–‘ìí™” ëª¨ë¸ ì‚°ì¶œë¬¼ + `.pte` + ì„±ëŠ¥ ë¦¬í¬íŠ¸

- [ ] **ëŒ€í‘œì ì¸ ìš©ëŸ‰ íšŒìˆ˜ ëŒ€ìƒ**
  - `checkpoints/distillation/phase1_*.pt`, `checkpoints/distillation/phase2_*.pt` ì¤‘ "ì¤‘ê°„ step" íŒŒì¼ë“¤ (ê° ~1.1GB)
  - `logs/distillation/vis/*.png` (ì‹œê°í™” ì´ë¯¸ì§€)
  - TensorBoard ì´ë²¤íŠ¸ íŒŒì¼(`logs/distillation/**/events.*`) (ìƒì„±ëœ ê²½ìš°)
  - (ë¹„ë””ì˜¤ ì¦ë¥˜ ì¢…ë£Œ í›„, ì¬í•™ìŠµ ê³„íšì´ ì—†ê³  ìš©ëŸ‰ì´ ê¸‰í•  ë•Œ) `data/sa_v/cached_features/*.pt` (~12GB)

- [ ] **ì •ë¦¬ ìŠ¤í¬ë¦½íŠ¸ (ê¸°ë³¸: dry-run)**
  - ìŠ¤í¬ë¦½íŠ¸: `scripts/prune_artifacts.py`
  - ì˜ˆì‹œ:
    ```bash
    # ë¬´ì—‡ì´ ì–¼ë§ˆë‚˜ ì§€ì›Œì§€ëŠ”ì§€ ë¨¼ì € í™•ì¸
    python scripts/prune_artifacts.py \
      --prune-distillation-ckpt --prune-vis --prune-tensorboard \
      --keep-last-n 1

    # ì‹¤ì œ ì‚­ì œ
    python scripts/prune_artifacts.py \
      --apply \
      --prune-distillation-ckpt --prune-vis --prune-tensorboard \
      --keep-last-n 1

    # (ì„ íƒ) êµì‚¬ ìºì‹œê¹Œì§€ ì‚­ì œ
    python scripts/prune_artifacts.py --apply --prune-cache
    ```

---

## 4ë‹¨ê³„: ì–‘ìí™” (Quantization with TorchAO)

> **ëª©í‘œ**: ëª¨ë¸ í¬ê¸° ì¶”ê°€ ì••ì¶• ë° NPU ì—°ì‚° ê°€ì†.
> FP16 â†’ Int4/Int8 ë³€í™˜ìœ¼ë¡œ ëª¨ë¸ í¬ê¸° 1/4, ì¶”ë¡  ì†ë„ 2ë°° ì´ìƒ í–¥ìƒ.
> **ì •í™•ë„ ê¸°ì¤€**: FP16 ëŒ€ë¹„ mIoU 2% ì´ë‚´ í•˜ë½.

### 4.1 ê°€ì¤‘ì¹˜ ì–‘ìí™” (Weight-Only Quantization â€” Int4 Group-wise)

- [x] **TorchAO Int4 Group-wise ì–‘ìí™” ì ìš©** âœ… (ì½”ë“œ ì™„ì„±)
  - `quantize_model.py --mode int4`: Int4WeightOnlyConfig(group_size=128)
  - ë¯¼ê° ë ˆì´ì–´ ë³´í˜¸: iou_head, dot_product_scoring, perceiver_resampler, memory_cross_attn (14 Linear â†’ FP16 ìœ ì§€)
  - ì–‘ìí™” ëŒ€ìƒ: 79 Linear layers (RepViT, MobileCLIP, DETR, Mask Decoder)
  - `should_quantize` filter_fnìœ¼ë¡œ ì„ íƒì  ì–‘ìí™” ì ìš©

- [x] **ì–‘ìí™” í›„ ëª¨ë¸ í¬ê¸° í™•ì¸** âœ… (ì½”ë“œ ì™„ì„±)
  - `quantize_model.py --mode compare`: FP16 vs Int4 vs Int8+Int4 ë¹„êµí‘œ ì¶œë ¥
  - ëª¨ë¸ í¬ê¸°, mIoU, Presence F1, ì¶”ë¡  ì‹œê°„ ë¹„êµ

### 4.2 ë™ì  í™œì„±í™” ì–‘ìí™” (Dynamic Activation Quantization â€” Int8)

- [x] **TorchAO Int8 Dynamic í™œì„±í™” ì–‘ìí™” ì ìš©** âœ… (ì½”ë“œ ì™„ì„±)
  - `quantize_model.py --mode int8_int4`: Int8DynamicActivationInt4WeightConfig()
  - ë™ì¼í•œ ë¯¼ê° ë ˆì´ì–´ ë³´í˜¸ ì ìš©

### 4.3 ì–‘ìí™” ì¸ì§€ í•™ìŠµ (QAT â€” Quantization-Aware Training)

- [x] **QAT ì ìš© ì—¬ë¶€ íŒë‹¨** âœ… (ì½”ë“œ ì™„ì„±)
  - `quantize_model.py --mode compare` ê²°ê³¼ì—ì„œ mIoU 2% ì´ˆê³¼ í•˜ë½ ì‹œ ê²½ê³  ì¶œë ¥

- [x] **QAT í•™ìŠµ ì‹¤í–‰** âœ… (ì½”ë“œ ì™„ì„±)
  - `train_qat.py`: TorchAO QATConfig(step="prepare") â†’ í•™ìŠµ â†’ QATConfig(step="convert")
  - SA-1B ë°ì´í„°ì…‹ìœ¼ë¡œ 1~2 ì—í¬í¬ fine-tuning (lr=1e-5, warmup=100)
  - Phase 2 distillation loss (output-only) ì‚¬ìš©
  - ì „ì²´ ëª¨ë¸ unfreeze + fake quant â†’ ì–‘ìí™” ë…¸ì´ì¦ˆ ì ì‘

### 4.4 ì–‘ìí™” ì •í™•ë„ ê²€ì¦

- [x] **SA-1B ê²€ì¦ ì„¸íŠ¸ í‰ê°€** âœ… (ì½”ë“œ ì™„ì„±)
  - SA1BAssessmentDataset: SA-1B ë§ˆì§€ë§‰ Nì¥ì„ ê²€ì¦ ì„¸íŠ¸ë¡œ ì‚¬ìš©
  - mIoU (GreedyMatcher ë§¤ì¹­), Presence F1, ì¶”ë¡  ì‹œê°„ ì¸¡ì •
  - RLE ë””ì½”ë”© (compressed + uncompressed í˜•ì‹ ì§€ì›)
  - NaN/Inf ì¶œë ¥ ê²€ì¦

- [x] **ë¯¼ê° ë ˆì´ì–´ Mixed Precision** âœ… (êµ¬í˜„ ì™„ë£Œ)
  - iou_head (3 Linear), dot_product_scoring (4 Linear), perceiver_resampler (6 Linear), memory_cross_attn (1 Linear)
  - ì´ 14ê°œ Linear ë ˆì´ì–´ FP16 ìœ ì§€, ë‚˜ë¨¸ì§€ 79ê°œ ì–‘ìí™”

> **ì‹¤í–‰ ëŒ€ê¸°**: Phase 2 distillation ì™„ë£Œ í›„ ì‹¤í–‰
> ```bash
> # Step 1: PTQ ë¹„êµ
> python quantize_model.py --mode compare --num-val 200
>
> # Step 2: mIoU 2% ì´ˆê³¼ ì‹œë§Œ
> python train_qat.py --mode int4 --epochs 2
> ```

---

## 5ë‹¨ê³„: ExecuTorch ë°°í¬ (Deployment)

> **ëª©í‘œ**: ì–‘ìí™”ëœ Mobile-SAM 3 ëª¨ë¸ì„ iOS/Androidì—ì„œ ì‹¤ì‹œê°„ êµ¬ë™.
> PyTorch ë„¤ì´í‹°ë¸Œ ê²½ë¡œ(ExecuTorch)ë¥¼ í†µí•´ NPU ê°€ì† ë°”ì´ë„ˆë¦¬(.pte) ìƒì„±.

### 5.1 ExecuTorch Lowering Pipeline

- [ ] **Step 1: Export (ë‚´ë³´ë‚´ê¸°)**
  ```python
  import torch
  from torch.export import export

  # ATen ì—°ì‚°ì ë‹¨ìœ„ë¡œ ê·¸ë˜í”„ ìº¡ì²˜
  exported_model = export(model, example_inputs)
  ```

- [ ] **Step 2: To Edge (ì—£ì§€ ë³€í™˜)**
  ```python
  from executorch.exir import to_edge

  # Edge Dialect IRë¡œ ë³€í™˜ (ë¶ˆí•„ìš” ì—°ì‚° ì œê±°, ë©”ëª¨ë¦¬ ë ˆì´ì•„ì›ƒ ìµœì í™”)
  edge_model = to_edge(exported_model)
  ```

- [ ] **Step 3: Partition & Delegate (ë¶„í•  ë° ìœ„ì„)**
  - NPU ì—°ì‚°: Conv, MatMul ë“± â†’ CoreML/QNN íŒŒí‹°ì…”ë„ˆë¡œ ìœ„ì„
  - CPU í´ë°±: ë³µì¡í•œ ì œì–´ íë¦„, DETR ë™ì  ì¿¼ë¦¬ ì²˜ë¦¬ â†’ XNNPACK
  ```python
  # í”Œë«í¼ë³„ íŒŒí‹°ì…”ë„ˆ ì ìš© (ì•„ë˜ 5.2, 5.3ì—ì„œ ìƒì„¸)
  edge_model = edge_model.to_backend(partitioner)
  ```

- [ ] **Step 4: Memory Planning & .pte ìƒì„±**
  ```python
  # ì •ì  ë©”ëª¨ë¦¬ í• ë‹¹ (ëŸ°íƒ€ì„ ë™ì  malloc ì˜¤ë²„í—¤ë“œ ì œê±°)
  et_program = edge_model.to_executorch()

  with open("mobile_sam3.pte", "wb") as f:
      f.write(et_program.buffer)
  ```

### 5.2 iOS: CoreML Backend (ANE ê°€ì†)

- [ ] **CoreML íŒŒí‹°ì…”ë„ˆ ì„¤ì •**
  ```python
  from executorch.backends.apple.coreml.partition import (
      CoreMLPartitioner, CoreMLCompileSpec
  )
  import coremltools as ct

  partitioner = CoreMLPartitioner(
      compile_spec=CoreMLCompileSpec(
          compute_units=ct.ComputeUnit.ALL,  # CPU + GPU + NPU(ANE) ëª¨ë‘ í™œìš©
          precision=ct.Precision.FLOAT16
      )
  )
  ```

- [ ] **SDPA ì—°ì‚°ì ë§¤í•‘ í™•ì¸**
  - `torch.nn.functional.scaled_dot_product_attention` â†’ CoreML ë ˆì´ì–´ ë§¤í•‘
  - ANEì—ì„œ Multi-Head Attention íš¨ìœ¨ì  ì‹¤í–‰ ë³´ì¥

- [ ] **iOS .pte íŒŒì¼ ìƒì„± ë° ë°ìŠ¤í¬íƒ‘ ê²€ì¦**

- [ ] **Xcode í”„ë¡œì íŠ¸ í†µí•©** (Swift/C++)
  - ExecuTorch iOS ë¼ì´ë¸ŒëŸ¬ë¦¬ íƒ‘ì¬
  - ëª¨ë¸ ë°”ì´ë„ˆë¦¬(.pte) ë²ˆë“¤ í¬í•¨
  - A18 Pro ANE 35 TOPS í™œìš© (FP16/Int8 ìµœì í™”)

### 5.3 Android: QNN Backend (Hexagon NPU)

- [ ] **QNN íŒŒí‹°ì…”ë„ˆ ì„¤ì •**
  ```python
  from executorch.backends.qualcomm.partition import QnnPartitioner

  partitioner = QnnPartitioner(
      # Snapdragon 8 Elite HTP(Hexagon Tensor Processor) íƒ€ê²Ÿ
      # ì–‘ìí™”ëœ ëª¨ë¸ ì „ë‹¬í•˜ì—¬ DSP ê°€ì† í™œì„±í™”
  )
  ```

- [ ] **QNN ê°œë°œ í™˜ê²½ êµ¬ì¶•**
  - Qualcomm AI Hub Docker ì´ë¯¸ì§€ í™œìš© ê¶Œì¥
  - QNN SDK ì„¤ì¹˜ ë° í™˜ê²½ ì„¤ì •

- [ ] **Android .pte íŒŒì¼ ìƒì„± ë° ê²€ì¦**

- [ ] **Android Studio í”„ë¡œì íŠ¸ í†µí•©** (Kotlin/JNI)
  - ExecuTorch Android ë¼ì´ë¸ŒëŸ¬ë¦¬ íƒ‘ì¬
  - ëª¨ë¸ ë°”ì´ë„ˆë¦¬(.pte) ì—ì…‹ í¬í•¨
  - Snapdragon 8 Elite NPU 45+ TOPS í™œìš©

### 5.4 ì˜¨ë””ë°”ì´ìŠ¤ í”„ë¡œíŒŒì¼ë§ ë° ì„±ëŠ¥ ìµœì í™”

- [ ] **ì¶”ë¡  ì§€ì—°ì‹œê°„(Latency) ì¸¡ì •**
  - ì´ë¯¸ì§€: ë‹¨ì¼ í”„ë ˆì„ ì¶”ë¡  ì‹œê°„ (ëª©í‘œ: < 100ms)
  - ë¹„ë””ì˜¤: FPS (ëª©í‘œ: > 15 FPS)

- [ ] **ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í”„ë¡œíŒŒì¼ë§**
  - í”¼í¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì •
  - Perceiver Resampler ë©”ëª¨ë¦¬ ê³ ì •(K=64) í™•ì¸
  - ì¥ì‹œê°„ ë¹„ë””ì˜¤ì—ì„œ OOM ë°œìƒ ì—¬ë¶€ í…ŒìŠ¤íŠ¸

- [ ] **NPU í™œìš©ë¥  í™•ì¸**
  - iOS: Xcode Instruments â†’ Neural Engine í™œìš©ë¥ 
  - Android: Snapdragon Profiler â†’ Hexagon NPU í™œìš©ë¥ 
  - CPU í´ë°± ë¹„ìœ¨ ìµœì†Œí™” (ëª©í‘œ: < 10% ì—°ì‚°ë§Œ CPU)

- [ ] **ë³‘ëª© êµ¬ê°„ ìµœì í™”**
  - NPU ë¯¸ì§€ì› ì—°ì‚°ì ì‹ë³„ ë° ëŒ€ì²´
  - í…ì„œ íƒ€ì¼ë§(Tiling) ìµœì í™” (Hexagon TCM í™œìš©)
  - ì „ì²˜ë¦¬/í›„ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ GPU ê°€ì†

- [ ] **ìµœì¢… ì„±ëŠ¥ ë¦¬í¬íŠ¸ ì‘ì„±**
  | ì§€í‘œ | ëª©í‘œê°’ |
  |------|--------|
  | ëª¨ë¸ í¬ê¸° | < 50MB (.pte) |
  | ì´ë¯¸ì§€ ì¶”ë¡  | < 100ms |
  | ë¹„ë””ì˜¤ FPS | > 15 FPS |
  | mIoU (SA-Co) | êµì‚¬ ëŒ€ë¹„ 85%+ |
  | í”¼í¬ ë©”ëª¨ë¦¬ | < 500MB |
  | NPU í™œìš©ë¥  | > 90% |

---

## ì°¸ê³ : ë©”ëª¨ë¦¬ ê´€ë¦¬ ì „ëµ (24GB UMA)

> ê°œë°œ í™˜ê²½(Mac Mini M4 Pro)ì—ì„œì˜ ë©”ëª¨ë¦¬ ê´€ë¦¬ ì§€ì¹¨

- ëª¨ë¸ ë¡œë“œ (FP16): ~1.7GB
- KV Cache ë° Activation: ë¹„ë””ì˜¤ ì²˜ë¦¬ ì‹œ ìˆ˜ GB~ì‹­ìˆ˜ GB
- ì‹œìŠ¤í…œ ì˜¤ë²„í—¤ë“œ (macOS + ê°œë°œ ë„êµ¬): ~4~6GB
- **Batch Size = 1 ìœ ì§€** (ìŠ¤ì™€í•‘ ë°©ì§€)
- **Activity Monitorì—ì„œ Memory Pressure ë…¸ë€ìƒ‰ ë¯¸ë§Œ ìœ ì§€**
- **ë¶ˆí•„ìš”í•œ ë¸Œë¼ìš°ì € íƒ­ ë‹«ê¸°** (2~4GB í™•ë³´ ê°€ëŠ¥)

---

## ì°¸ê³ : í•µì‹¬ íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

| ì—ëŸ¬ | ì›ì¸ | í•´ê²° |
|------|------|------|
| `RuntimeError: Triton packages are not available` | NVIDIA ì „ìš© ê²½ë¡œ ì‚¬ìš© | `device="mps"` ëª…ì‹œ ì§€ì •, CUDA import ë¹„í™œì„±í™” |
| `pin_memory` ê´€ë ¨ ì˜¤ë¥˜ | MPS + pin_memory ë¹„í˜¸í™˜ | `DataLoader(pin_memory=False)` |
| NPU í´ë°± ê³¼ë‹¤ | ë¯¸ì§€ì› ì—°ì‚°ì | ì—°ì‚°ì ëŒ€ì²´ ë˜ëŠ” XNNPACK íŒŒí‹°ì…”ë‹ |
| OOM (Out of Memory) | ë©”ëª¨ë¦¬ ë±…í¬ ì„ í˜• ì¦ê°€ | Perceiver Resampler (K=64) ì ìš© |
